---
layout: post
title: Azure Storage Data Movement Uploader in Five Commits
date: 2016-01-11 18:44:43.000000000 -05:00
type: post
published: true
status: publish
categories:
- Azure
- Development
tags:
- API
- Azure
- Azure Storage
- Git
- Github
- SDK
meta:
  _edit_last: '1'
  _syntaxhighlighter_encoded: '1'
  _jetpack_related_posts_cache: a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1457920216;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:691;}i:1;a:1:{s:2:"id";i:1491;}i:2;a:1:{s:2:"id";i:248;}}}}
  _wpas_done_all: '1'
author:
  login: adminian
  email: ian.philpot@microsoft.com
  display_name: adminian
  first_name: ''
  last_name: ''
---
<p><img src="{{ site.baseurl }}/assets/filecopy.gif" alt="" /></p>
<p>I have two goals: <strong>Learn more about Git</strong> and<strong> learn about DMLib</strong>. <strong>Azure Storage Data Movement Library</strong> (DMLib) has the same performance and exposes the core functionalities of AzCopy and this is an awesome super(ficial) review of one of its methods. It’s not that in-depth, but it does a good job of checking just beneath the hood.  I’m going to try to use commits and branches to tell the story, the real goal of this post. This is a simple example to start out, but if you like this method of storytelling using Git, or if you don’t, leave a comment!</p>
<p>On January 7th, Microsoft posted to the Azure Blog that there is a new release of DMLib and it <strong>rev’ed to 0.2.0</strong>. Lot’s of work went into giving us the <strong>ability to move directories around</strong>: up to Azure, around in Azure, you get the idea. I’ve played a lot with AzCopy, on which this particular set of classes is based. I’ve not played at all with DMLib, but it’s a big deal, so let’s have a look.</p>
<p>We have a repository to work from hosted at: <a title="https://github.com/devdash/AzureStorageDataMovementTester" href="http://ianp.co/1kZ0qoG">https://github.com/devdash/AzureStorageDataMovementTester</a>. I want you to first focus on the branches, there are four: ParallelOps, ProgressManager, SvcPointMgr, and of course master. We’ll follow the Branches and commits in this order:</p>
<ol>
<li>Master</li>
<li>ProgressManager</li>
<li>ParallelOps</li>
<li>SvcPointMgr</li>
</ol>
<p>Let’s take a step back and have a look at the <a href="http://ianp.co/1kZ1rNr">documentation</a> for DMLib, the reasons for these branches will become apparent. The docs have you Upload a blob. This upload creates a progress tracker and configures parallel operations. There is a best practice section in the docs that talks about increasing the HTTP connections limit using the ServicePointManager class. Now have a look at the commits for the “last” branch…</p>
<p><a href="http://devian.co/wp-content/uploads/2016/01/image.png"><img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;" title="image" src="{{ site.baseurl }}/assets/image_thumb.png" alt="image" width="610" height="389" border="0" /></a></p>
<p>As you see, the <strong>commits/branches mirror the examples the documentation</strong> gives us. It also let’s us switch between different configurations of the uploader, so that we can test the functionality of what each piece of code adds. Cool! I think this is a neat way of using Git, not just for keeping code safe, but also as a learning tool.</p>
<p>Enough of the Git horse play, let us look at the code! The “Initial Commit” is nothing more than GitHub creating the repo, so we’ll skip over that and only four commits left. Starting with the master branch I simply added the project with a bit of bits already laid down. This code will <strong>take a ZIP file</strong> found in the project and <strong>upload it to blob storage</strong>. We will use the app.config file and drop the connection string in, hit F5 and it uploads the file. It then pauses and waits for you to verify everything has worked as expected. If it has you can the hit any key and it will clean up all the changes it just made to you storage account and close. Simple.</p>
<h1>The Code that Does the Dang Thing</h1>
<p>From the <strong>Master</strong> Branch:</p>
<p>[code lang="csharp"]</p>
<p>private static async Task BlobUploadTest()<br />
        {<br />
            string sourceFilename = &quot;Downloads.zip&quot;;<br />
            string destinationFilename = &quot;Downloads_blockblob.zip&quot;;</p>
<p>            CloudBlob destinationBlob = GetCloudBlob(ContainerName, destinationFilename, BlobType.BlockBlob);<br />
            UploadOptions options = new UploadOptions();<br />
            options.ContentType = &quot;application/zip&quot;;</p>
<p>            try<br />
            {<br />
                await TransferManager.UploadAsync(sourceFilename, destinationBlob, options, null);<br />
            }<br />
            catch (Exception ex)<br />
            {<br />
                Console.WriteLine(ex.Message);<br />
            }</p>
<p>            Console.WriteLine(&quot;File {0} is uploaded to {1} successfully.&quot;, sourceFilename, destinationBlob.Uri.ToString());<br />
        }</p>
<p>[/code]</p>
<p>This code first calls the <em>GetCloudBlob</em> method that uses the standard <a href="http://ianp.co/1VZOMr5">Azure Storage SDK</a> to create a container in your storage account. I’m not going to go into detail about that. The next part we just set the content type for file we are uploading. These are just standard <a href="http://ianp.co/1W05y9y">MIME types</a>. Finally, we move on to the actual upload. Here we use the aptly named static method <em>UploadAsync </em>that is hung off the class <em>TransferManager</em>. It takes three parameters: Source File, Destination Blob, Options, and Transfer context. The Transfer context for now is null. It does what you think, uploads a file to blob storage. Press F5 to run this bit of code, it does do much but before you press any key, which deletes everything we just did from Azure, have a look at the blobs in your storage account, do you see a new container? A new file? It took me <strong>15784 milliseconds</strong> to upload the file…</p>
<h1>But I want Progress Tracking</h1>
<p>From the <strong>ProgressManager</strong> Branch:</p>
<p>[code lang="csharp"]</p>
<p>private static async Task BlobUploadTest()<br />
        {<br />
            string sourceFilename = &quot;Downloads.zip&quot;;<br />
            string destinationFilename = &quot;Downloads_blockblob.zip&quot;;</p>
<p>            CloudBlob destinationBlob = GetCloudBlob(ContainerName, destinationFilename, BlobType.BlockBlob);<br />
            UploadOptions options = new UploadOptions();<br />
            options.ContentType = &quot;application/zip&quot;;</p>
<p>            TransferContext context = new TransferContext();<br />
            context.ProgressHandler = new Progress&lt;TransferProgress&gt;((progress) =&gt;<br />
            {<br />
                Console.WriteLine(&quot;Bytes Uploaded: {0}&quot;, progress.BytesTransferred);<br />
            });</p>
<p>            try<br />
            {<br />
                await TransferManager.UploadAsync(sourceFilename, destinationBlob, options, context, CancellationToken.None);<br />
            }<br />
            catch (Exception ex)<br />
            {<br />
                Console.WriteLine(ex.Message);<br />
            }</p>
<p>            Console.WriteLine(&quot;File {0} is uploaded to {1} successfully.&quot;, sourceFilename, destinationBlob.Uri.ToString());<br />
        }</p>
<p>[/code]</p>
<p>This code news up the transfer context. Then news up the <em>Progress</em> generic class with type <em>TransferProgress </em>and passes a delegate as a parameter to the constructor. The <em>Progress</em> class hangs off the <em>ProgressHandler</em>  on the aforementioned context. Basically it will write to the console how many bytes have been transferred. Which is a great opportunity to do welcomed binary maths. We also update the <em>UploadAsync</em> method to include the TransferContext. Now when run you will see the byte count being uploaded. Everything else work the same. It took me <strong>18458 milliseconds</strong> to upload the file…</p>
<h1>Now Do It in Parallel</h1>
<p>From the <strong>ParallelOps</strong> Branch:</p>
<p>[code lang="csharp"]</p>
<p>private static async Task BlobUploadTest()<br />
        {<br />
            string sourceFilename = &quot;Downloads.zip&quot;;<br />
            string destinationFilename = &quot;Downloads_blockblob.zip&quot;;</p>
<p>            CloudBlob destinationBlob = GetCloudBlob(ContainerName, destinationFilename, BlobType.BlockBlob);<br />
            UploadOptions options = new UploadOptions();<br />
            options.ContentType = &quot;application/zip&quot;;</p>
<p>            TransferManager.Configurations.ParallelOperations = 64;</p>
<p>            TransferContext context = new TransferContext();<br />
            context.ProgressHandler = new Progress&lt;TransferProgress&gt;((progress) =&gt;<br />
            {<br />
                Console.WriteLine(&quot;Bytes Uploaded: {0}&quot;, progress.BytesTransferred);<br />
            });</p>
<p>            try<br />
            {<br />
                await TransferManager.UploadAsync(sourceFilename, destinationBlob, options, context, CancellationToken.None);<br />
            }<br />
            catch (Exception ex)<br />
            {<br />
                Console.WriteLine(ex.Message);<br />
            }</p>
<p>            Console.WriteLine(&quot;File {0} is uploaded to {1} successfully.&quot;, sourceFilename, destinationBlob.Uri.ToString());<br />
        }</p>
<p>[/code]</p>
<p>This code is one line and sets the ParallelOperations to 64. This should decrease the time to upload. Previous test was 18458 milliseconds and mine ran this time in <strong>16042 milliseconds</strong>.</p>
<h1>Best Practice: Set the HTTP Connection Limit</h1>
<p>From the <strong>SvcPointMgr</strong> Branch:</p>
<p>[code lang="csharp"]</p>
<p>private static async Task BlobUploadTest()<br />
        {<br />
            string sourceFilename = &quot;Downloads.zip&quot;;<br />
            string destinationFilename = &quot;Downloads_blockblob.zip&quot;;</p>
<p>            CloudBlob destinationBlob = GetCloudBlob(ContainerName, destinationFilename, BlobType.BlockBlob);<br />
            UploadOptions options = new UploadOptions();<br />
            options.ContentType = &quot;application/zip&quot;;</p>
<p>            ServicePointManager.DefaultConnectionLimit = Environment.ProcessorCount * 8;<br />
            TransferManager.Configurations.ParallelOperations = 64;</p>
<p>            TransferContext context = new TransferContext();<br />
            context.ProgressHandler = new Progress&lt;TransferProgress&gt;((progress) =&gt;<br />
            {<br />
                Console.WriteLine(&quot;Bytes Uploaded: {0}&quot;, progress.BytesTransferred);<br />
            });</p>
<p>            try<br />
            {<br />
                await TransferManager.UploadAsync(sourceFilename, destinationBlob, options, context, CancellationToken.None);<br />
            }<br />
            catch (Exception ex)<br />
            {<br />
                Console.WriteLine(ex.Message);<br />
            }</p>
<p>            Console.WriteLine(&quot;File {0} is uploaded to {1} successfully.&quot;, sourceFilename, destinationBlob.Uri.ToString());<br />
        }</p>
<p>[/code]</p>
<p>This code is also a one liner and sets the DefaultConnectionLimit of the ServicePoint Object (ServicePointManager Class) to 8 times the processor count. Per the documentation this is one of the learnings from the folks behind AzCopy. My upload time is now <strong>15863 milliseconds</strong>.</p>
<p>&nbsp;</p>
<p>This post lays out a simple file upload to Azure Blob storage using the Azure Storage Data Movement Library. Since it pulls from the learnings and code behind AzCopy you should take it seriously. Spend some time with the samples and start <strong>planning</strong> for this library in your applications.</p>
